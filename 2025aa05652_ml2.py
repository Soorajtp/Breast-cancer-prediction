# -*- coding: utf-8 -*-
"""2025aa05652_ML2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ap8K0vGVzCk7BriLBWR8yip4UnyX4TwB

Breast Cancer Wisconsin (Diagnostic) Data Set-Multi variate-30 features
Predict whether the cancer is benign or malignant-https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic
Dataset info-Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/
"""

# Implementation of Decision Tree,kNN,Naive Bayes,Random Forest (Ensemble),XGBoost (Ensemble) on the heart diseas data set and prediction comparison

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#load data
from google.colab import drive
drive.mount('/content/drive')

# create Data frame
import pandas as pd

url = "/content/drive/MyDrive/ML2/Breast Cancer Wisconsin.csv"
df = pd.read_csv(url)

print(df.columns)
print(df.head())
print(df.shape)

# Drop useless columns
df = df.drop(columns=["id", "Unnamed: 32"])

# Check target before mapping
print("Before mapping:\n", df["diagnosis"].value_counts())

# Map target to 0/1
df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

# Check after mapping
print("After mapping:\n", df["diagnosis"].value_counts())
print("Unique values:", df["diagnosis"].unique())

print(df.head())
print(df.shape)

X=df.drop('diagnosis',axis=1)
Y=df['diagnosis']

#show data set
print(df.describe())

from sklearn.preprocessing import StandardScaler
import pandas as pd

scaler = StandardScaler()
X_scaled_array = scaler.fit_transform(X)   # this is a NumPy array

# Convert back to DataFrame using original column names
X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns)

print(X_scaled.describe())

print(df.columns)
print(df.head())

X = df.drop("diagnosis", axis=1)
y = df["diagnosis"]

print("X shape:", X.shape)
print("y shape:", y.shape)
print("y unique:", y.unique())

import numpy as np
from scipy import stats

z_scores = np.abs(stats.zscore(X))

# Rows where any feature has z-score > 3
outlier_rows = np.where(z_scores > 3)

print("Number of rows with any outlier:", len(set(outlier_rows[0])))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)
print("y_train unique:", y_train.unique())
print("y_test unique:", y_test.unique())

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef
import pandas as pd

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(eval_metric="logloss", random_state=42)
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)

    results.append([name, acc, auc, prec, rec, f1, mcc])

results_df = pd.DataFrame(
    results,
    columns=["Model", "Accuracy", "AUC", "Precision", "Recall", "F1", "MCC"]
)

print(results_df)

import matplotlib.pyplot as plt

metrics_names = ["Accuracy", "Precision", "Recall", "F1", "MCC"]
metrics_values = [acc, prec, rec, f1, mcc]

fig, ax = plt.subplots()
ax.bar(metrics_names, metrics_values)
ax.set_ylim(0, 1)
ax.set_title("Performance Metrics")
ax.set_ylabel("Score")

plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

for idx, (name, model) in enumerate(models.items()):
    # Train
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[idx])
    axes[idx].set_title(name)
    axes[idx].set_xlabel("Predicted")
    axes[idx].set_ylabel("Actual")

plt.tight_layout()
plt.show()

"""Logistic Regression performs very well, indicating that the dataset is close to linearly separable after normalization.

Random Forest and XGBoost achieve the highest accuracy and MCC, showing the benefit of ensemble methods.

KNN also performs competitively but slightly worse than ensemble methods.

Naive Bayes and Decision Tree show comparatively lower performance, possibly due to their simplifying assumptions and tendency to overfit or underfit

This project implements and compares six machine learning classifiers (Logistic Regression, Decision Tree, KNN, Naive Bayes, Random Forest, and XGBoost) on the Breast Cancer Wisconsin dataset. The models are evaluated using Accuracy, AUC, Precision, Recall, F1-score, and MCC, and deployed via a Streamlit web app with interactive visualizations.

Project Overview

This repository contains an end-to-end machine learning project for breast cancer classification using the Breast Cancer Wisconsin (Diagnostic) dataset from UCI/Kaggle. The dataset consists of 569 samples with 30 numerical features extracted from digitized images of fine needle aspirate (FNA) of breast masses.

The project implements and compares six classification models: Logistic Regression, Decision Tree, K-Nearest Neighbors, Naive Bayes, Random Forest, and XGBoost. The models are evaluated using multiple performance metrics including Accuracy, AUC, Precision, Recall, F1-score, and Matthews Correlation Coefficient (MCC).

A Streamlit web application is provided to allow interactive model selection, visualization of performance metrics, confusion matrix, ROC curve, and prediction on uploaded CSV test data.
"""